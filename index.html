<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
      content="ViSaRL is a simple approach for incorporating human-annotated saliency to learn representations for visual control tasks.">
    <meta name="keywords"
      content="ViSaRL, Visual Reinforcement Learning, Representation Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ViSaRL: Visual Reinforcement Learning Guided by Human Saliency</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async
      src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

    <script
      src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <a class="navbar-item" href="https://aliang8@github.io">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <!-- <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div> -->
          </div>
        </div>

      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">ViSaRL: Visual Reinforcement
                Learning Guided by Human Saliency</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://aliang8.github.io">Anthony Liang</a>,</span>
                <span class="author-block">
                  <a href="https://jessethomason.com/">Jesse Thomason</a>,</span>
                <span class="author-block">
                  <a href="https://ebiyik.github.io/">Erdem Biyik</a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">University of Southern California</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2403.10940"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2403.10940"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/aliang8/visarl"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Dataset Link. -->
                  <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
                </div>

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <p style="text-align: center; font-size: 24px; margin-bottom: 0px;">
      <strong><em>Can human visual attention help agents perform visual control
          tasks?</em></strong>
    </p>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Training robots to perform complex control tasks from high-dimensional
                pixel input using reinforcement learning (RL) is sample-inefficient,
                because image observations are comprised primarily of task-irrelevant
                information.
                By contrast, humans are able to visually attend to task-relevant objects
                and areas.
                Based on this insight, we introduce <span
                  style="color: maroon; font-weight: bold">Visual Saliency-Guided
                  Reinforcement
                  Learning (ViSaRL).</span>
              </p>
              <p>
                Using ViSaRL to learn visual representations significantly improves the
                <b><i>success rate, sample efficiency, and
                    generalization</i></b> of an RL agent on diverse tasks including
                DeepMind
                Control benchmark, robot manipulation
                in simulation and on a real robot.
                We present approaches for incorporating saliency into both CNN and
                Transformer-based encoders.
                We show that visual representations learned using ViSaRL are robust to
                various sources of visual perturbations
                including perceptual noise and scene variations.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser Picture -->
    <section class="hero teaser" style="margin-bottom: -50px;">
      <div class="hero-body">
        <div class="container is-max-desktop" style="text-align: center;">
          <div style="text-align: center;">
            <img src="./static/images/teaser.png" alt="teaser" style="width: 80%;">
          </div>
          ViSaRL trains a saliency prediction model from a few human-annotated saliency
          maps. This model is used to augment an offline image dataset with saliency. A
          visual encoder is pretrained with the dataset and used during downstream
          policy learning to generate latent representations of the agentâ€™s
          observations.
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3">ViSaRL</h2>

            <div class="content has-text-justified">
              <p>
                <b>The key idea of ViSaRL is to train a visual encoder using both RGB
                  and saliency inputs and an RL policy that operates over
                  lower dimensional image representations.</b>
                By using a multimodal autoencoder trained using a self-supervised
                objective, our learned representations attend to the
                most salient parts of an image for downstream task learning making them
                robust to visual distractors.
                To circumvent the expensive process of manually annotating saliency
                maps, we train a state-of-the-art saliency predictor
                using only a few human-annotated examples to pseudo-label RGB
                observations with saliency.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="hero-body">
        <div class="container is-max-desktop" style="text-align: center;">
          <div>
            <img src="./static/images/mmae_model.png" alt="mmae" style="width: 80%;">
          </div>
          MultiMAE employs a self-supervised objective in which masked patches for both
          input modalities are reconstructed given only the visible patches. The
          pretrained model is frozen and used for extracting representations during
          policy learning.
        </div>
      </div>
    </section>

    <!-- Simulation Experiment and Results -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3">Simulation Experiments</h2>

            <div class="content has-text-justified">
              <p>
                We show quantitative results of our approach with two different
                encoder backbones, CNN and Transformer, across multiple simulated
                environments including the Meta-World manipulation and DMC
                benchmarks.
              </p>
            </div>
            <div class="hero teaser is-max-desktop container">
              <div>
                <img src="./static/images/results_plot.png" alt="mmae">
              </div>
              <p>
                <b>Learning curves</b> for four robot manipulation tasks in Meta-World
                evaluated by task success rate. <b>(Top)</b> CNN encoder methods.
                <b>(Bottom)</b> Transformer encoder methods.
              </p>
            </div>

            <div class="content has-text-justified" style="margin-left: 80px">
              <p
                style="color: maroon; font-weight: bold; font-size: 20px; margin-top: 20px">
                Insights:
              </p>
              <ol>
                <li>Saliency input improves downstream task success rates.</li>
                <li>A saliency channel achieves the best task success rate for CNN
                  encoder.</li>
                <li>Training encoder with saliency improves RGB-only success rates at
                  inference time.</li>
                <li>Using saliency in both pretraining and inference yields the best
                  performance.</li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Saliency Analysis -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Content and Image Side by Side -->
        <div class="columns is-vcentered">
          <!-- Text Column -->
          <div class="column">
            <h2 class="title is-3" style="font-size: 1.5rem;">Masked
              Reconstruction with
              MultiMAE</h2>

            <div class="content has-text-justified is-6">
              MultiMAE predictions for different random masks. We visualize the
              masked predictions for RGB observation from each of the four tasks. For
              each input image, we randomly sample three different masks from a uniform
              distribution between RGB and saliency. Even when there are a few unmasked
              patches from one modality, the reconstructions are still very accurate due
              to cross-modal interaction.
            </div>
          </div>
          <div class="column is-7 teaser">
            <div style="text-align: center;">
              <img src="./static/images/mask_inputs.png" alt="masked_inputs"
                style="width: 100%;">
              Masked input images and RGB / saliency reconstruction by MultiMAE.
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Quality of the Saliency Model -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Content and Image Side by Side -->
        <div class="columns is-vcentered">
          <!-- Text Column -->
          <div class="content has-text-justified">
            <h2 class="title is-3" style="font-size: 1.5rem;">Quality
              of Saliency Model</h2>

            <p>
              Zero-shot evaluation using a pretrained PiCANet and even
              state-of-the-art Vision Language Models fail to correctly identify the
              task-relevant regions of the image necessitating the use of
              human-annotation to guide the saliency training. Thus, we train a new
              saliency model for each task.

              We apply random vertical and horizontal flip for data augmentation to
              prevent overfitting on our small dataset.
              We measure the quality of the saliency model on two evaluation metrics
              following prior work: <b>F-measure score (<i>F<sub>&zeta;</sub></i>)</b>
              and <b>Mean Absolute Error
                (MAE)</b>.
              On our test dataset, we obtain an <i>F<sub>&zeta;</sub></i>
              score of 0.78 Â± 0.02 and MAE of 0.004 Â± 0.003 averaged across all tasks,
              consistent with results reported in prior work.
            </p>
          </div>
          <div class="column is-6 teaser">
            <div style="text-align: center;">
              <img src="./static/images/saliency.png" alt="masked_inputs"
                style="width: 100%;">
              Saliency map predictions for held out test images in real world setup.
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Real World Results and Videos -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3">Real World Experiments</h2>

            <div class="hero-body teaser">
              <div class="container is-max-desktop">
                <div style="flex: 1;">
                  <img src="./static/images/real_world_envs.png" alt="real_world_envs"
                    style="width: 60%;">
                </div>
                <p><b>Evaluation Tasks</b>. Four Meta-World (top) simulation tasks and
                  four
                  real-robot tabletop manipulation tasks (bottom).
                </p>
              </div>
            </div>

            <div style="display: flex; align-items: center; margin-top: 30px;">
              <!-- Text Container (40% Width) -->
              <div style="flex: 0 0 40%; text-align: left; margin-right: 20px;">
                <h3 class="title is-3" style="font-size: 1.5rem;">Real World
                  Results</h3>

                <p>
                  <b>ViSaRL scales to real-robot tasks and is robust to distractor
                    objects</b>.
                  Even on the easier Pick Apple task, using saliency augmented
                  representations, <span
                    style="color: blue; font-weight: bold">RGB+Saliency</span>, improves
                  the success rate over <span
                    style="color: green; font-weight: bold">RGB</span>.
                  On tasks with distractor objects and longer horizon tasks such as Put
                  Apple in Bowl, ViSaRL representations nearly <b>doubles</b> the
                  success
                  rate.
                </p>
              </div>
              <!-- Image Container (60% Width) -->
              <div style="flex: 0 0 60%; text-align: center;">
                <img src="./static/images/real_world_results.png"
                  alt="real_world_results"
                  style="width: 100%; vertical-align: middle;">
                Task success rates on 10 evaluation rollouts.
              </div>
            </div>

            <div style="display: flex; align-items: center; margin-top: 50px;">
              <!-- Text Container (30% Width) -->
              <div style="flex: 0 0 30%; text-align: left; margin-right: 20px;">
                <h3 class="title is-3" style="font-size: 1.5rem;">Robot Policy
                  Architecture</h3>

                <p>
                  The downstream policy is trained using standard imitation learning.
                  The state representation is the visual embedding and the robot
                  proprioceptive information (e.g., joint positions).
                  This yields a 271-dimensional state representation.
                  The policy is an LSTM with 256-dimensional hidden states.
                  The final hidden state of the LSTM is processed by a 2-layer MLP to
                  predict a continuous action.
                  The action space, <strong>A &isin; R<sup>7</sup></strong>, consists of
                  <strong>&Delta;(x, y, z, &phi;, &theta;, &psi;)</strong>, and a
                  continuous scalar value for gripper speed.
                </p>
              </div>

              <!-- Image Container (70% Width) -->
              <div style="flex: 0 0 70%; text-align: center;">
                <img src="./static/images/robot_il_model.png" alt="robot_policy"
                  style="width: 100%; vertical-align: middle;">
              </div>
            </div>

            <!-- Videos -->
            <div
              style="display: flex; flex-direction: column; align-items: center; margin-top: 50px;">
              <div>
                <h3 class="title is-3"
                  style="font-size: 1.5rem; margin-bottom: 50px;">Real World
                  Rollouts (Left: <span
                    style="color: green; font-weight: bold">RGB-only</span>, Right:
                  <span
                    style="color: blue; font-weight: bold">RGB
                    +
                    Saliency)</span></h3>
              </div>
              <div class="columns is-centered"
                style="display: flex; align-items: center; width: 100%; max-width: 1200px; margin-bottom: 30px;">
                <div
                  style="flex: 1; text-align: left; padding-left: 100px; padding-right: 20px;">
                  <h3 style="color: maroon; font-weight: bold; font-size: 20px;">Pick Up
                    Apple</h3>
                </div>
                <div style="flex: 2; display: flex; gap: 20px;">
                  <img
                    src="./static/videos/pickup_apple/rgb_only/rgb_only_fail_eval_0.gif"
                    alt="pick up apple fail" width="250" />
                  <img
                    src="./static/videos/pickup_apple/rgb+saliency/rgb+saliency_success_eval_0.gif"
                    alt="pick up apple success" width="250" />
                </div>
              </div>

              <div class="columns is-centered"
                style="display: flex; align-items: center; width: 100%; max-width: 1200px; margin-bottom: 30px;">
                <div
                  style="flex: 1; text-align: left; padding-left: 100px; padding-right: 20px;">
                  <h3 style="color: maroon; font-weight: bold; font-size: 20px;">Pick Up
                    Red Block with Distractor Objects</h3>
                </div>
                <div style="flex: 2; display: flex; gap: 20px;">
                  <img
                    src="./static/videos/pickup_red_block_with_distractor_objects/rgb_only/rgb_only_fail_eval_0.gif"
                    alt="pick up red block fail" width="250" />
                  <img
                    src="./static/videos/pickup_red_block_with_distractor_objects/rgb+saliency/rgb+saliency_success_eval_0.gif"
                    alt="pick up red block success" width="250" />
                </div>
              </div>

              <div class="columns is-centered"
                style="display: flex; align-items: center; width: 100%; max-width: 1200px; margin-bottom: 30px;">
                <div
                  style="flex: 1; text-align: left; padding-left: 100px; padding-right: 20px;">
                  <h3 style="color: maroon; font-weight: bold; font-size: 20px;">Put
                    Bread
                    on Plate</h3>
                </div>
                <div style="flex: 2; display: flex; gap: 20px;">
                  <img
                    src="./static/videos/put_bread_on_plate/rgb_only/rgb_only_fail_eval_2.gif"
                    alt="put bread on plate fail" width="250" />
                  <img
                    src="./static/videos/put_bread_on_plate/rgb+saliency/rgb+saliency_success_eval_0.gif"
                    alt="put bread on plate success" width="250" />
                </div>
              </div>

              <div class="columns is-centered"
                style="display: flex; align-items: center; width: 100%; max-width: 1200px; margin-bottom: 30px;">
                <div
                  style="flex: 1; text-align: left; padding-left: 100px; padding-right: 20px;">
                  <h3 style="color: maroon; font-weight: bold; font-size: 20px;">Put
                    Apple
                    in Bowl with Distractor Objects</h3>
                </div>
                <div style="flex: 2; display: flex; gap: 20px;">
                  <img
                    src="./static/videos/put_apple_in_bowl_with_distractor_objects/rgb_only/rgb_only_fail_eval_0.gif"
                    alt="put apple in bowl fail" width="250" />
                  <img
                    src="./static/videos/put_apple_in_bowl_with_distractor_objects/rgb+saliency/rgb+saliency_success_eval_0.gif"
                    alt="put apple in bowl success" width="250" />
                </div>
              </div>
            </div>

          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{liang2024visarl,
  author    = {Liang, Anthony and Thomason, Jesse and B{\i}y{\i}k, Erdem},
  title     = {ViSaRL: Visual Reinforcement Learning Guided by Human Saliency},
  journal   = {IROS},
  year      = {2024},
}</code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <a class="icon-link"
            href="./static/videos/nerfies_paper.pdf">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/aliang8"
            class="external-link"
            disabled>
            <i class="fab fa-github"></i>
          </a>
        </div>
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
              <p>
                Website design borrowed from <a
                  href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

  </body>
</html>
